{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RoFWzNeaTOTu",
        "Gqqrdcp-iO5d",
        "agyFKLVu-DFY",
        "yjzOe-Wt61bY",
        "w0ZwHSJvCshR",
        "Z-Gj3sHwJSmB",
        "HjqdNIlkojmR",
        "mgysKg2TxgWh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Karim ElDakroury"
      ],
      "metadata": {
        "id": "-PHXzh7_ncJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ELMo: Embeddings from Language Models \n",
        "![](https://get.whotrades.com/u4/photoDE6C/20647654315-0/blogpost.jpeg)"
      ],
      "metadata": {
        "id": "2WpIeeeM1J6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will implement a deep lstm-based model for contextualized word embeddings - ELMo. Your tasks are as following: \n",
        "\n",
        "- Preprocessing (20 points)\n",
        "- Implementation of ELMo model (30 points)\n",
        "  - 2-layer BiLSTM (15 points)\n",
        "  - Highway layers (5 points) [link](https://paperswithcode.com/method/highway-layer) [paper](https://arxiv.org/pdf/1507.06228.pdf) [code](https://github.com/allenai/allennlp/blob/9f879b0964e035db711e018e8099863128b4a46f/allennlp/modules/highway.py#L11)\n",
        "  - CharCNN embeddings (10 points) [paper](https://arxiv.org/pdf/1509.01626.pdf)\n",
        "- Report metrics and loss using tensorbord/comet or other tool.  (10 points)\n",
        "- Evaluate on movie review dataset (20 pts)\n",
        "- Compare the performance with BERT model (10 pts)\n",
        "- Clean and documented code (10 points)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "\n",
        "*   Use Pytorch\n",
        "*   Cheating will result in 0 points\n",
        "\n",
        "\n",
        "ELMo paper: https://arxiv.org/pdf/1802.05365.pdf\n",
        "\n",
        "Possible datasets:\n",
        "- [WikiText-103](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
        "- Any monolingual dataset from [WMT](https://statmt.org/wmt22/translation-task.html)"
      ],
      "metadata": {
        "id": "fjrI02kE2Zfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and preprocessing\n",
        "Preprocess the english monolingual data (20 points):\n",
        "- clean\n",
        "- split to train and validation\n",
        "- tokenize\n",
        "- create vocabulary, convert words to numbers. [vocab](https://pytorch.org/text/stable/vocab.html#id1)\n",
        "- pad sequences\n",
        "\n",
        "Use these tutorials [one](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) and [two](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*UPirqwpBWnNmcwoUjfZZIA.png)"
      ],
      "metadata": {
        "id": "RoFWzNeaTOTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install allennlp\n",
        "# !pip install unidecode"
      ],
      "metadata": {
        "id": "8D2MmBI0hGk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import Counter\n",
        "from torch import nn, optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as torch_vocab\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "FLy123MALhFK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset\n",
        "\n",
        "Wiki-103 is already split\n"
      ],
      "metadata": {
        "id": "Gqqrdcp-iO5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.utils import download_from_url, extract_archive\n",
        "\n",
        "dataset_url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
        "dataset_path = download_from_url(dataset_url)\n",
        "test_path, valid_path, train_path = extract_archive(dataset_path)\n",
        "print(test_path, valid_path, train_path, sep='\\n')"
      ],
      "metadata": {
        "id": "FPqduVrh6rYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e21189-4130-427f-ed3e-a851420c7069"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.data/wikitext-2/wiki.test.tokens\n",
            "/content/.data/wikitext-2/wiki.valid.tokens\n",
            "/content/.data/wikitext-2/wiki.train.tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Path('/content/.data/wikitext-2/wiki.train.tokens').read_text()"
      ],
      "metadata": {
        "id": "qgPu3vLI_IyW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare tokenizer"
      ],
      "metadata": {
        "id": "agyFKLVu-DFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize(text):\n",
        "    header_token = '( \\n = [^=]*[^=] = \\n )'\n",
        "    text = re.split(header_token, text)\n",
        "    text = [x for x in text[2::2]]\n",
        "    subheader_token = '( \\n = = .* = = \\n )'\n",
        "    punc_token = '(\\'s)|(\\n)|(?:(?<!unk)([^a-zA-Z0-9\\s\\.])(?!unk))'\n",
        "    cleaned_text = []\n",
        "    for word in text:\n",
        "      data = re.sub(subheader_token, '', word)    \n",
        "      data = re.sub(punc_token, '', data)\n",
        "      data = re.sub(' +', ' ', data.strip())\n",
        "      data = data.lower()\n",
        "      cleaned_text.append(data)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "6GjBP1WJD2-M"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build vocab"
      ],
      "metadata": {
        "id": "9QL8MjMtLzt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unk_tok = '<unk>'\n",
        "pad_tok = '<pad>'\n",
        "bos_tok = '<bos>'\n",
        "eos_tok = '<eos>'\n",
        "\n",
        "specials = [pad_tok, unk_tok, bos_tok, eos_tok]\n",
        "\n",
        "def get_sentences(data):\n",
        "  sentences = []\n",
        "  tokenizer = get_tokenizer('spacy', language='en')\n",
        "  for article in data:\n",
        "    article_sentences = re.split(' +\\. +', article)\n",
        "    for sent in article_sentences:\n",
        "      sent = re.sub('\\.', '', sent).strip()\n",
        "      sentences.append(sent.split())\n",
        "  return sentences\n",
        "\n",
        "def build_vocab(data):        \n",
        "  counter = Counter()\n",
        "  for s in data:\n",
        "    counter.update(s)\n",
        "  return torch_vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
      ],
      "metadata": {
        "id": "57-A7SIrDwJM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_articles = sanitize(train_data)\n",
        "sentences = get_sentences(cleaned_articles)\n",
        "# cut the list of sentences\n",
        "sentences = random.sample(sentences, 1000)\n",
        "vocab = build_vocab(sentences)\n",
        "print(sentences[0])\n",
        "print(vocab(['<unk>', 'book', 'the', '<bos>', '<eos>', '<pad>']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76z9Gd4AD_-w",
        "outputId": "b64c23b7-8dd9-4e38-f197-be3f0d952d9b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'upgrade', 'proposal', 'proved', 'to', 'be', 'very', 'unpopular', 'with', 'north', 'kingstown', 'residents', 'who', 'lived', 'on', 'the', 'affected', 'local', 'roads']\n",
            "[0, 1401, 4, 2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding, creating tensors"
      ],
      "metadata": {
        "id": "ml-dykTuNT3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentence_len = 50\n",
        "batch_size = 32\n",
        "\n",
        "def padding(sentences, max_len=max_sentence_len):\n",
        "  sent_tensor = torch.zeros(len(sentences), max_len, dtype=torch.long, device=device)\n",
        "  masks = torch.zeros(len(sentences), max_len, dtype=torch.long, device=device)\n",
        "  return masks, sent_tensor"
      ],
      "metadata": {
        "id": "a1j6MO69NUpx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, sent_tensor = padding(sentences)\n",
        "print(sent_tensor.size(), masks.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9mfi1H3Ricx",
        "outputId": "69213606-2188-420e-9c1e-725ea43cd622"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 50]) torch.Size([1000, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert words to numbers\n"
      ],
      "metadata": {
        "id": "xh-e-lMfRW_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def converting_words2numbers(setntences, vocab, masks, sent_tensor, max_len=max_sentence_len):\n",
        "  lengths = []\n",
        "  for idx, sentence in enumerate(sentences):        \n",
        "    sent_len = min(max_len - 2, len(sentence))\n",
        "    \n",
        "    masks[idx][:sent_len+2] = torch.ones(sent_len+2, device=device)        \n",
        "    sent_tensor[idx][1:sent_len+1] = torch.tensor(vocab(sentence[:sent_len]), dtype=torch.long, device=device)\n",
        "    sent_tensor[idx][0] = vocab[bos_tok]\n",
        "    sent_tensor[idx][sent_len+1] = vocab[eos_tok]\n",
        "    lengths.append(sent_len+2)\n",
        "  \n",
        "  lengths = torch.tensor(lengths, dtype=torch.long, device=device)\n",
        "  return lengths\n",
        "\n",
        "lengths = converting_words2numbers(sentences, vocab, masks, sent_tensor)\n",
        "\n",
        "print(lengths.size())\n",
        "\n",
        "dataset = TensorDataset(sent_tensor, masks, lengths)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4LkCdgyRZ4d",
        "outputId": "066cb5e3-0959-4091-adb7-6b27a446d1b8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model - learning embeddings\n",
        "Read chapter 3 from the [paper](https://arxiv.org/pdf/1802.05365.pdf)\n",
        "\n",
        "Implement this model with \n",
        "- 2 BiLSTM layers,\n",
        "- CharCNN embeddings,\n",
        "- Highway layers,\n",
        "- out-of-vocabulary words handling\n",
        "\n",
        "Plot the training and validation losses over the epochs (iterations)\n",
        "\n",
        "Use the [implementation](https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*3_wsDpyNG-TylsRACF48yA.png)\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*8pG54o28pbD2L0dv5THL-A.png)"
      ],
      "metadata": {
        "id": "wU_JDeZ6TTx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "https://github.com/gazelle93/charCNN\n",
        "\n",
        "\n",
        "https://github.com/ankurbanga/Language-Models/tree/master/ELMo\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/pytorch-elmo-844d2391a0b2#b484"
      ],
      "metadata": {
        "id": "Dm80VukaVVqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character embeddings"
      ],
      "metadata": {
        "id": "1VWsLO8XSdQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_chars_per_token = 40\n",
        "emb_dim = 16\n",
        "filters= [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]]\n",
        "output_dim = 512\n",
        "num_highway = 2\n",
        "\n",
        "bow_tok = '<bow>'\n",
        "eow_tok = '<eow>'\n",
        "\n",
        "special_chars = [bow_tok, eow_tok]\n",
        "\n",
        "def char_dict(special_chars=specials+special_chars):\n",
        "  chars = string.ascii_lowercase + string.digits\n",
        "  char_dict = {}\n",
        "  idx = 0\n",
        "  for char in special_chars:\n",
        "    char_dict[char] = idx\n",
        "    idx += 1\n",
        "  for char in chars:\n",
        "    char_dict[char] = idx\n",
        "    idx += 1\n",
        "  return char_dict"
      ],
      "metadata": {
        "id": "-rMHqRERSq4T"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CharCNN"
      ],
      "metadata": {
        "id": "g92jPkSPSruT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, num_chars, emb_dim, filters):\n",
        "        super(CharCNN, self).__init__()\n",
        "        self.embeddings = nn.Embedding(num_chars, emb_dim)\n",
        "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels=emb_dim,\n",
        "                                     out_channels=num_f,\n",
        "                                     kernel_size=width,\n",
        "                                     bias=True) for width, num_f in filters])\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        convs = []\n",
        "        initial_embeddings = self.embeddings(inputs)\n",
        "\n",
        "        for conv_layer in self.conv_layers:\n",
        "            convolved = conv_layer(torch.transpose(initial_embeddings, 1, 2))\n",
        "            convolved, _ = torch.max(convolved, dim=-1)\n",
        "            convolved = self.activation(convolved)\n",
        "            convs.append(convolved)\n",
        "\n",
        "        return torch.cat(convs, dim=-1)"
      ],
      "metadata": {
        "id": "lBxTyj5ESvmD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Highway layer"
      ],
      "metadata": {
        "id": "7I_ejBEUSwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Highway(nn.Module):\n",
        "    def __init__(self, dim, n_highway):\n",
        "        super(Highway, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [torch.nn.Linear(dim, dim * 2) for _ in range(n_highway)]\n",
        "        )\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer.bias[dim:].data.fill_(1)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      current_input = inputs\n",
        "      for layer in self.layers:\n",
        "        projected_input = layer(current_input)\n",
        "        linear_part = current_input\n",
        "\n",
        "        nonlinear_part, gate = projected_input.chunk(2, dim=-1)\n",
        "        nonlinear_part = self.activation(nonlinear_part)\n",
        "        gate = torch.sigmoid(gate)\n",
        "        current_input = gate * linear_part + (1 - gate) * nonlinear_part\n",
        "\n",
        "      return current_input"
      ],
      "metadata": {
        "id": "AhsE1JEhWj6A"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elmo encoder\n",
        "preparing ELMO inputs from CharCNN, Highway, and projection layers"
      ],
      "metadata": {
        "id": "xRvHogsxS-BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Projection(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Projection, self).__init__()\n",
        "    self.layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return self.layer(inputs)\n",
        "\n",
        "class CharIndexer(nn.Module):\n",
        "  def __init__(self, vocab, max_char, char):\n",
        "      super(CharIndexer, self).__init__()\n",
        "      self.vocab = vocab\n",
        "      self.char = char\n",
        "      self.max_char = max_char\n",
        "\n",
        "  def forward(self, sent):\n",
        "    embedding = []\n",
        "\n",
        "    for token in sent:\n",
        "      word = self.vocab.lookup_token(token)\n",
        "\n",
        "      embedded_word = [self.char[pad_tok] for _ in range(self.max_char)]\n",
        "      \n",
        "      embedded_word[0] = self.char[bow_tok]\n",
        "      if word != pad_tok:\n",
        "        if word == bos_tok or word == eos_tok or word == unk_tok:\n",
        "          embedded_word[1] = self.char[word]\n",
        "          embedded_word[2] = self.char[eow_tok]\n",
        "        \n",
        "        else:\n",
        "          for idx, char in enumerate(word[:self.max_char - 2]):\n",
        "            embedded_word[idx+1] = self.char[char]\n",
        "            last_idx = idx+2\n",
        "          \n",
        "          embedded_word[last_idx] = self.char[eow_tok]\n",
        "      embedding.append(embedded_word)\n",
        "      return embedding\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, output_dim=output_dim, max_char=max_chars_per_token, num_highway=num_highway):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.output_dim=output_dim\n",
        "      self.filters = filters\n",
        "      self.char = char_dict()\n",
        "      self.char_embeddings = CharIndexer(vocab, max_char, self.char)\n",
        "      self.charCNN = CharCNN(len(self.char), emb_dim, self.filters)\n",
        "      \n",
        "      dim = sum([x[1] for x in self.filters])\n",
        "      self.highway = Highway(dim, num_highway)\n",
        "      self.projection = Projection(dim, output_dim)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "      max_sent_len = sentences.size()[0]\n",
        "      batch_size = sentences.size()[1]\n",
        "      embeddings = torch.zeros((batch_size, max_sent_len, self.output_dim), dtype=torch.float, device=device)\n",
        "      \n",
        "      for i, sent in enumerate(sentences.t()):\n",
        "        emb = torch.tensor(self.char_embeddings(sent), dtype=torch.long, device=device)\n",
        "        emb = self.charCNN(emb)\n",
        "        emb = self.highway(emb)\n",
        "        emb = self.projection(emb)\n",
        "        embeddings[i] = emb\n",
        "      return torch.permute(embeddings, (1, 0, 2))"
      ],
      "metadata": {
        "id": "1bEPHINfR5Vx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 biLSTM layers"
      ],
      "metadata": {
        "id": "lesgMPh8TJYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class biLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(biLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = embedding\n",
        "        USE_CUDA = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "        self.forwardLSTM = nn.LSTM(hidden_size, \n",
        "                                         hidden_size, \n",
        "                                         n_layers, \n",
        "                                         dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.backwardLSTM = nn.LSTM(hidden_size, \n",
        "                                         hidden_size, \n",
        "                                         n_layers, \n",
        "                                         dropout=(0 if n_layers == 1 else dropout))\n",
        "        \n",
        "    def forward(self, input_seq, input_lengths, initial_states=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = embedded.to(device)\n",
        "        MAX_LEN = embedded.size()[0]\n",
        "        batch_size = embedded.size()[1]\n",
        "        outputs = torch.zeros(MAX_LEN, batch_size, 2, self.hidden_size, device=self.device)\n",
        "        hidden_states = torch.zeros(self.n_layers * 2, MAX_LEN, batch_size, self.hidden_size, device=self.device)\n",
        "        \n",
        "        if not initial_states:\n",
        "            initial_states = (torch.zeros(self.n_layers, 1, self.hidden_size, device=self.device), torch.zeros(self.n_layers, 1, self.hidden_size, device=self.device))\n",
        "        \n",
        "        for batch_n in range(batch_size):\n",
        "            b_sentence = embedded[:,batch_n, :]\n",
        "            length = input_lengths[batch_n]\n",
        "            \n",
        "            sentence = self.drop(b_sentence[:length,:])\n",
        "            hidden_forward_state, cell_forward_state = initial_states\n",
        "            hidden_backward_state, cell_backward_state = initial_states\n",
        "            \n",
        "            for t in range(length):\n",
        "                output, (hidden_forward_state, cell_forward_state) = self.forwardLSTM(sentence[t].view(1, 1, -1), (hidden_forward_state, cell_forward_state))\n",
        "                outputs[t, batch_n, 0, :] = output[0, 0, :]\n",
        "                hidden_states[:self.n_layers, t, batch_n, :] = hidden_forward_state[:, 0, :]\n",
        "                \n",
        "            for t in range(length):\n",
        "                output, (hidden_backward_state, cell_backward_state) = self.backwardLSTM(sentence[length - t - 1].view(1, 1, -1), (hidden_backward_state, cell_backward_state))\n",
        "                outputs[length - t - 1, batch_n, 1, :] = output[0, 0, :]\n",
        "                hidden_states[self.n_layers:, length - t - 1, batch_n, :] = hidden_backward_state[:, 0, :]\n",
        "                \n",
        "        return outputs, hidden_states, embedded"
      ],
      "metadata": {
        "id": "czWwyVVsTMQ2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELMO model\n"
      ],
      "metadata": {
        "id": "o91gK_c8T7wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ELMo(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, embedding, vocab_size, n_layers=1, dropout=0):\n",
        "        super(ELMo, self).__init__()\n",
        "        USE_CUDA = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.biLSTM = biLSTM(hidden_size, embedding, n_layers, dropout)\n",
        "        self.W = nn.Parameter(torch.tensor([1/(2*n_layers + 1) for i in range(2*n_layers + 1)], requires_grad=True, device=self.device))\n",
        "        self.gamma = nn.Parameter(torch.ones(1, requires_grad=True, device=self.device))\n",
        "        self.dense = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, mask, initial_states=None):\n",
        "        biLSTM_outputs, hidden_states, embedded = self.biLSTM(input_seq, input_lengths, initial_states)\n",
        "        concat_hidden_with_embedding = torch.cat((embedded.unsqueeze(0), hidden_states), dim=0)\n",
        "        scaled_embedding = torch.zeros(*embedded.size(), device=self.device)\n",
        "        \n",
        "        for i in range(2*self.n_layers + 1):\n",
        "            w = self.W[i]\n",
        "            layer = concat_hidden_with_embedding[i]\n",
        "            scaled_embedding = scaled_embedding + w * layer\n",
        "        \n",
        "        scaled_embedding *= self.gamma\n",
        "        perm_scaled_embedding = scaled_embedding.permute(1, 0, 2)\n",
        "        projection_size = (perm_scaled_embedding.size()[0], perm_scaled_embedding.size()[2])\n",
        "        dense_layer_input = torch.zeros(projection_size, dtype=torch.float, device=device)\n",
        "        for idx, sent_emb in enumerate(perm_scaled_embedding):\n",
        "          dense_layer_input[idx] = sent_emb[input_lengths[idx] - 1]\n",
        "        \n",
        "        vocab_projection = self.dense(dense_layer_input)\n",
        "        prop = self.softmax(vocab_projection)\n",
        "        \n",
        "        return prop, scaled_embedding, biLSTM_outputs"
      ],
      "metadata": {
        "id": "WOQjR8pnT-fj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "WoQzdgsNThF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "clip = 50.0\n",
        "\n",
        "def make_confusion_matrix(true, pred):\n",
        "    K = len(np.unique(true))\n",
        "    result = np.zeros((K, K))\n",
        "\n",
        "    for i in range(len(true)):\n",
        "        result[true[i]][pred[i]] += 1\n",
        "    return result\n",
        "\n",
        "def train_model(model, model_optimizer, dataloader):\n",
        "    model.to(device)\n",
        "    train_true = []\n",
        "    train_pred = []\n",
        "    train_loss = []\n",
        "    \n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        curr_true = []\n",
        "        curr_pred = []\n",
        "        model.train()\n",
        "        \n",
        "        for batch in dataloader:\n",
        "            model_optimizer.zero_grad()\n",
        "            inp_seq, masks, lengths = batch\n",
        "            prediction, elmo_embedding, biLSTM_outputs = model(inp_seq.t(), lengths, masks.t())\n",
        "          \n",
        "            true_pred = []\n",
        "            for i, length in enumerate(lengths):\n",
        "              true_pred.append(inp_seq[i][length - 1])\n",
        "\n",
        "            true_pred = torch.tensor(true_pred, dtype=torch.long, device=device)\n",
        "            loss_func = nn.CrossEntropyLoss()\n",
        "            loss = loss_func(prediction, true_pred)\n",
        "            loss = loss.to(device)\n",
        "            train_loss.append(loss.item() * lengths.size()[0])\n",
        "            \n",
        "            loss.backward(retain_graph=True)\n",
        "            \n",
        "            _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            model_optimizer.step()\n",
        "        print(\"\\nloss:\", loss.item())\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "eWWW44XaTf2l"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "keyboard intruption"
      ],
      "metadata": {
        "id": "rCHQuCGdXZgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 512\n",
        "n_layers = 2\n",
        "lr = 2e-3\n",
        "\n",
        "model = ELMo(dim, Encoder(vocab), len(vocab), n_layers=n_layers, dropout=0.4)\n",
        "model_optimizer = optim.Adam(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "SeYHdFMBTxn8"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = train_model(model, model_optimizer, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AoapAjswnDYc",
        "outputId": "6015ca0b-7c92-463f-fa2e-2b20539d68db"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [00:57<18:05, 57.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loss: 7.673977851867676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [01:02<19:52, 62.78s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-3ba931a118df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-57b5a62e5942>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_optimizer, dataloader)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate your embeddings model on IMDB movie reviews dataset (sentiment analysis) \n",
        "[Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "Preprocess data\n",
        "\n",
        "Disable training for ELMo, it will produce 5 embeddings for each word, add trainable parameters $\\gamma^{task}$ and $s^{task}_j$\n",
        "\n",
        "Don't forget metric plots"
      ],
      "metadata": {
        "id": "WQ4HsuafA5sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "import unidecode\n",
        "\n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def train_test_split(dataset, test_split):\n",
        "    train_len = int((1 - test_split) * len(dataset))\n",
        "    test_len = len(dataset) - train_len\n",
        "    train, test = torch.utils.data.random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
        "    return train, test\n",
        "\n",
        "def load_data(filename) -> Tuple[List[str], List[int]]:\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            line = unidecode.unidecode(line.lower())\n",
        "            line = re.sub(r\"([^a-zA-Z0-9\\s])\", \"\", line)\n",
        "            word_list = [word.strip() for word in line.split() if word.strip() != '']\n",
        "            inputs.append(word_list[:-1])\n",
        "            targets.append(int(word_list[-1]))\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "Ywnyt-T4Yb0z"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elmo_data(sentences, targets):\n",
        "  # pad the sequence\n",
        "  padded = []\n",
        "  max_len = 0\n",
        "  for sent in sentences:\n",
        "    max_len = max(max_len, len(sent))\n",
        "  for sent in sentences:\n",
        "    tmp_sent = [pad_tok for _ in range(max_len)]\n",
        "    tmp_sent[:len(sent)] = sent\n",
        "    padded.append(tmp_sent)\n",
        "  \n",
        "  # get embeddings and create batches\n",
        "  embeddings = batch_to_ids(padded)\n",
        "  labels = torch.tensor(targets, dtype=torch.float, device=device)\n",
        "  dataset = TensorDataset(embeddings, labels)\n",
        "  train, test = train_test_split(dataset, 0.25)\n",
        "  return DataLoader(train, batch_size, shuffle=True), DataLoader(test, batch_size), max_len\n",
        "\n",
        "class ElmoClassifier(nn.Module):\n",
        "  def __init__(self, max_sent_len, options_file, weight_file):\n",
        "    super(ElmoClassifier, self).__init__()\n",
        "    self.elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
        "    self.word_cls = nn.Linear(1024, 1)\n",
        "    self.sent_cls = nn.Linear(max_sent_len, 1)\n",
        "\n",
        "  def forward(self, character_ids):\n",
        "    output = self.elmo(character_ids)\n",
        "    embeddings = output['elmo_representations'][0]\n",
        "    word_cls = self.word_cls(embeddings)\n",
        "    return self.sent_cls(word_cls.squeeze(-1))\n",
        "\n",
        "def train_elmo(dataloader, model, optimizer, n_epochs=20):\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  for epoch in tqdm(range(n_epochs)):\n",
        "    for iteration, (embeddings, labels) in enumerate(dataloader):\n",
        "      embeddings = embeddings.to(device)\n",
        "      labels = labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(embeddings)\n",
        "      loss = criterion(logits.squeeze(-1), labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      acc = get_accuracy_from_logits(logits, labels)\n",
        "\n",
        "  print(\"\\nepoch {} complete. Loss : {} Accuracy : {}\".format(epoch + 1, loss.item(), acc))"
      ],
      "metadata": {
        "id": "2zxXsJX8YgbO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = load_data('data.txt')\n",
        "\n",
        "train_loader, test_loader, max_len = get_elmo_data(inputs, targets)\n",
        "model = ElmoClassifier(max_len, options_file, weight_file)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00002)"
      ],
      "metadata": {
        "id": "Nte-ryYybGCT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_elmo(train_loader, model, optimizer)"
      ],
      "metadata": {
        "id": "2KNZW4OEzb8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7264930a-c294-4e99-a3b7-998ded6f59a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:12<00:00,  6.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 20 complete. Loss : 0.7006427049636841 Accuracy : 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the results with BERT embeddings\n",
        "you can choose other bert model"
      ],
      "metadata": {
        "id": "DKhTvahJBcBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel, TrainingArguments, Trainer\n",
        "\n",
        "def get_bert_data(sentences, targets):\n",
        "  # tokenize dataset and create batches\n",
        "  bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "  data = [\" \".join(words_list) for words_list in sentences]\n",
        "  tokenized_data = bert_tokenizer(data, padding=\"max_length\")\n",
        "  embeddings = torch.tensor(tokenized_data['input_ids'])\n",
        "  attn_masks = torch.tensor(tokenized_data['attention_mask'])\n",
        "  labels = torch.tensor(targets, dtype=torch.float)\n",
        "  dataset = TensorDataset(embeddings, attn_masks, labels)\n",
        "  train, test = train_test_split(dataset, 0.25)\n",
        "  return DataLoader(train, batch_size, shuffle=True), DataLoader(eval, batch_size)\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False \n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        output = self.bert_layer(seq, attention_mask=attn_masks)\n",
        "        cont_reps = output.last_hidden_state\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "I0dMJv-wNvza"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bert(model, optimizer, train_loader, val_loader, n_epochs=20):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        for iteration, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(seq, attn_masks) # task: model forward pass\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        acc = get_accuracy_from_logits(logits, labels)\n",
        "    print(\"\\nIteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(iteration + 1, epoch + 1, loss.item(), acc))"
      ],
      "metadata": {
        "id": "T01PXfg7YjQq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = get_bert_data(inputs, targets)\n",
        "model = BertClassifier()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Rq0cBFYkzV",
        "outputId": "8bf76ce3-3593-45c5-ce05-174951870d4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_bert(model, optimizer, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGV1C7X0Yl3j",
        "outputId": "37fb121c-916c-42e7-d322-df8219e79632"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [08:48<00:00, 26.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 24 of epoch 20 complete. Loss : 0.6732714176177979 Accuracy : 0.7142857313156128\n"
          ]
        }
      ]
    }
  ]
}